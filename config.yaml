# Context Budget Optimizer Configuration

# Embedding Model
embedding:
  model_name: "all-MiniLM-L6-v2"  # Lightweight, fast model
  # Alternative: "all-mpnet-base-v2" for better quality (slower)

# Chunking Settings
chunking:
  chunk_size: 500  # Characters per chunk
  chunk_overlap: 50  # Overlap between chunks
  chunk_by_sentences: true  # Try to chunk at sentence boundaries

# Vector Database
vector_db:
  type: "chroma"  # Options: "chroma", "faiss"
  persist_directory: "./data/vector_db"
  collection_name: "context_chunks"

# Retrieval Settings
retrieval:
  top_k: 50  # Number of candidate chunks to retrieve

# Optimization Settings
optimization:
  default_budget: 2000  # Default token budget
  min_budget: 500  # Minimum allowed budget
  max_budget: 8000  # Maximum allowed budget
  reserve_tokens: 200  # Reserve tokens for prompt template and response
  relevance_weight: 1.0  # Weight for relevance score in value calculation

# Generation Settings
generation:
  model: "mistral-small"  # Mistral API model: mistral-tiny, mistral-small, mistral-medium, mistral-large-latest
  temperature: 0.7
  max_tokens: 1000  # Maximum tokens for response
  system_prompt: |
    You are a helpful assistant that answers questions based on the provided context.
    Use only the information from the context to answer. If the context doesn't contain
    enough information, say so clearly.

# Metadata Storage
metadata:
  db_path: "./data/metadata.db"  # SQLite database path

# Logging
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  log_file: "./data/app.log"

# Cache Settings
cache:
  embedding_dir: "./data/cache/embeddings"
  token_dir: "./data/cache/tokens"

# UI Settings
ui:
  show_progress: true  # Show progress bars for long operations
